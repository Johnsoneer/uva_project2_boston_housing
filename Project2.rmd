---
title: 'Project 2: Group 1 - Boston Housing Racial Bias'
author: "Lauren Bassett, Will Johnson, Anoop Nath, Aishwarya Pradhan"
date: "12/3/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
# install.packages("gridExtra")
# install.packages("ROCR")
library(gridExtra)
library(ROCR)
library(MASS)
#?BostonHousing2
```

# Executive Summary

The Boston Housing Data from the 1970 census, collected by David Harrison and Daniel L Rubinfeld, shows various statistical patterns. This is a standard dataset within R that contains 506 observations across 14 distinct variables, or factors. Each observation is a census "tract", or a neighborhood within a single county, along with several interesting measurments about that neighborhood.

Given the dataset, the variables, and the context of the data, this report attempts to find relationships between race and housing, if any, and the bias implications they present given the post-Civil Rights Act era in the United States. The main factor, the proportion of “blacks” by town was hypothesized to have a significant effect on the housing prices in Boston in 1970. For the purposes of the analysis that factor was transformed into a threshold of 26% - neighborhoods with a black population of 26% or greater would be deemed a “black neighborhood”, and those below that threshold would be “non-black” for the purpose of this analysis. Using a simplified factor makes it easier to identify any patterns when comparing black neighborhoods and their effect on housing prices. Other factors were also included in the analysis, based on their individual relationship to housing prices: distribution of housing per area, crime rate, housing room count (a proxy for size), and proportion of non-retail businesses, to name a few. 

We conducted Statistical analysis using a set of linear regression models to determine if there was a dependent relationship between housing price and any of the factors in the dataset, including the “black neighborhood” threshold created. The analysis showed no signs of dependent relationship between the factors. We found that a given neighborhood being “black” as stated by the threshold had a negative impact on the housing price. Moreover, the data suggests that the deficit in prices is around $3,000 lower simply for being a “black neighborhood”. 
 
In the second part of the analysis, a model was created that could potentially predict whether a neighborhood is in a black neighborhood given various inputs. In order to see the effectiveness of the model, the available data was split into two -- one for creating the model and another for testing its accuracy. The first model that was considered tried to predict the likelihood that a neighborhood was a black neighborhood using all available factors in our dataset. The results were mixed and showed that only two of the 13 factors  considered were needed to tell a black neighborhood apart. The two factors that were significant were the average home value and the average number of rooms in each house in the neighborhood.

Then, a model was created with only these two factors as inputs. The results were promising and showed that the model does have predictive value. The model shows that holding the average number of rooms in a neighborhood constant, increasing the average home value in a neighborhood by $1,000, the odds that the neighborhood is a black neighborhood is multiplied by 0.74. On the other hand, holding home value constant, increasing the average number of rooms in a neighborhood by one, the odds that the neighborhood is a black neighborhood is multiplied by 4.37. 

Overall, both models conclude that race plays an impact in housing prices in Boston in the 1970s. The model used suggested that there could be a difference in housing prices ranging from \$1,700 - \$4,300 for a black neighborhood, even in the presence of the other factors involved. Given that typical housing prices in a black neighborhood in Boston in the 1970s did not exceed \$15,000, this is a significant difference.

# Exploring our Dataset

### First 6 rows of data

```{r, echo=FALSE}
# Download our dataset
library(mlbench)
data(BostonHousing2)
data <- BostonHousing2
head(data)
```


This shows us the various avaiable features we can use for our model. We immediately identified a potential problem with one feature regarding race in boston housing.

## "Majority Black" response variable

The variable "b" is described in the census documents as...

> 1000(B - 0.63)^2 where B is the proportion of blacks by town
Initially, our goal was to categorize the data using a “majority black” binary variable. Based on the description, we thought the values would be between 0 and 1, or possibly 0 and 100. Instead, the variable is distributed on a quadratic curve. 

### Histogram of b-variable 

```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
# plot the distribution on the "b" variable. 
ggplot(data, aes(x = b))+
  geom_histogram(binwidth = 1)
```

The proportion variable ranges from 0 to 400, and most of the data has values on the upper end of the range. The data has been transformed so that higher values represent neighborhoods with lower black populations. 
	
If we try to reverse engineer the original B value (based on the formula b=1000(B-.63)2), it’s not possible to get the answer. The result of the previously mentioned formula is a quadratic curve, plotted below:

### Potential proportion of Black Population vs. b-variable
```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
# Using the dataset's documentation, plot potential values for proportions of black people
# against each value of `data$b`. 
props <- c()
i <- .01
for(x in 1:99){
    props <- c(props,1000*(i-.63)**2 )
    i = i + .01
}
parab <- data.frame(props,seq(.01, .99, by=.01))
colnames(parab) = c('x','y')
ggplot(data = parab, aes(x=y, y=x ))+
  geom_line()+
  labs(x = "Potential proportion of black people", y='b variable')
```

It is impossible to retrieve the original B variable because there are two possible values of B for each transformed variable. Neighborhoods with a black population higher than 26% become harder to determine. 
	

Some brief external research suggests that this transformation on the variable was set to create a pseudo-parabolic relationship to account for an initial drop in value when the proportion was too mixed-race. After 75% black, the effect was expected to rise again because of preference for a neighborhood of one’s own race or “self-segregation” [Harrison,Rubinfeld, 96].
	

Researcher Michael Carlisle, an assistant professor of Mathematics at CUNY, said:

> Harrison and Rubinfeld appear to have decided on a threshold of 63% at which to switch the regime of price decline to price increase (i.e. a so-called “ghetto threshold”) [Carlisle,1]
	
We will continue to follow the precedent they set in the 1970’s. We’ll assess if there is a relationship between where a home falls on that threshold to see its effect on the price of a home, as well as try to predict where on that threshold a house will fall for our logistic model.

Plugging our 26% threshold into that formula, we get:

$1000*(0.26 - 0.63)2=136.9$

so black_neighborhood = 1 if b <= 136.9

### What % of Census Tracts are considered "Black Neighborhoods"
```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
# code to create binary black_neighborhood variable
data <- data %>% 
           mutate(black_neighborhood = (b <= 136.9) * 1)
# what % of our neighborhoods are black neighborhoods by this definition? 
sum(data$black_neighborhood)/nrow(data)
```
About 7%. After generating the binary variable, we generated visualizations to see how the other variables interact. Specifically, we focused on the price variable. 

```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
data$black_neighborhood <- as.factor(data$black_neighborhood)
# plot our new variable and the price distributions across it. 
ggplot(data, aes(x= black_neighborhood, y=medv))+
  geom_boxplot(fill="light blue")
```


The visualizations above show a relationship between neighborhoods that are predominately black and the price of housing. Thus, we can assume that the black neighborhood indicator can be used as a predictor for housing prices. 




# Part 1 - EDA  : How much does the relative blackness of a neighborhood affect price? 

First, we will generate views to determine what other predictors affect the price of housing. We begin by looking at the distribtion of house prices across the entire dataset. 

```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
ggplot(data, aes(x=cmedv))+
  geom_histogram()+
  labs(title = "Distribution of Median Housing Values per Area")
```


The price of housing appears normally distributed. There are a few towns with noticably higher prices. These may be potential outliers in the dataset.  

Next, we generated scatterplots to describe the quantiative variables. 



```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
ggplot(data, aes(x = log(crim), y = cmedv))+
  geom_point(color = 'green2')+
  labs(title="Crime Rate (log) by Median Home Value",
       x = "Crime Rate - Log Scale")
```

It appears that as crime rate increases, the median value of the home decreases. However, most areas in the data set have low crime rates. Thus, we used a log scale for crime to better highlight this trend. It is important to note that the relationship between crime and housing prices is not constant, as there are some expensive areas in the data set that have relatively higher crime rates per capita. 


Next, we assess how the size of the home, approximated by the number of rooms, affects the median price. 


```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
ggplot(data, aes(x = rm, y = cmedv))+
  geom_point(color = 'green3')+
  labs(title="Room Count avg by Median Home Value",
       x = "Average Number of Rooms")
```

The relationship between number of rooms and median price is strong. However, there are some high-leverage points (by observation) where the median home value is around $50,000 but the average number of rooms is far lower. There may be other predictors that are influencing these high-leverage points.  

One possible confounding variable could be the industry proportion. We believe business neighborhoods with high expenses may also have relatively high crime rates.

```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
ggplot(data, aes(x = indus, y = cmedv))+
  geom_point(color = 'green4')+
  labs(title="Proportion of non-retail business by Median Home Value",
       x = "% of non-retail business acres per town")
```



```{r, echo=FALSE}
p1 <- ggplot(data, aes(x = nox, y = cmedv))+
      geom_point(color = 'purple1')+
      labs(title="NO2 concentration by Med. Value",
           x = "Nitric Oxides Conentration (parts per 10m)")
p2 <- ggplot(data, aes(x = dis, y = cmedv))+
      geom_point(color = 'purple2')+
      labs(title="Distance to Work by Med. Value",
           x = "Weighted Distances to Employment Centres")
p3 <- ggplot(data, aes(x = rad, y = cmedv))+
      geom_point(color = 'purple3')+
      labs(title="Highway Access by Med. Value",
           x = "Index of Accessibility to Highways")
p4 <- ggplot(data, aes(x = tax, y = cmedv, color=rad))+
      geom_point(alpha=.5)+
      labs(title="Property Tax Rate by Med. Value",
           x = "Property Tax rate per $10,000")
# Proportions
p5 <- ggplot(data, aes(x = ptratio, y = cmedv))+
      geom_point(color = 'orange2')+
      labs(title="Student-Teacher Ratio by Med. Value",
           x = "Studnets per Teacher")
p6 <- ggplot(data, aes(x = age, y = cmedv))+
      geom_point(color = 'orange3')+
      labs(title="Age by Med. Value",
           x = "% of homes build prior to 1940")
p7 <- ggplot(data, aes(x = lstat, y = cmedv))+
  geom_point(color = 'orange4')+
  labs(title="% of Lower Class by Med. Value",
       x = "% of 'Lower Status' Citizens")
# Boolean Variables
p8 <- ggplot(data, aes(x = chas, y = cmedv))+
      geom_boxplot(fill = "lightblue2")+
      labs(title="House By the River (y/n) by Med. Value",
           x = "Homes Line Charles River")
grid.arrange(p1,p2,p3,p4,
  ncol = 2,
  clip = TRUE
)
grid.arrange(p5,p6,p7,p8,
  ncol = 2,
  clip = FALSE
)
```

The views reveal key insights about our data set. The tax rate chart divides the dataset into two groups. The only other predictor that does this is the Index of Radial Highway access. Layering both together into the same chart via color, you can see that the group in the highest tax bracket is entirely made up of those areas with close access to the highways. Accessibility to highways seems to create a divide between groups in our dataset.

Other relationships of note are the positive relationship between proximity to the river on price, and the negative relationship between age of the home on price. Another metric of note is the indexed metric `lstat`, which measures the "Percentage of Lower Status of the Population". Looking further into the 1970 Census paper, it looks like this metric measures the following:


> Proportion of population that is lower status = 1/2 (proportion of adults without, some high school education and proportion of male workers classified as laborers). The logarithmic specification implies that socioeconomic status distinctions mean more in the upper brackets of society than in the lower classes. Source: 1970 U. S. Census. [Harrison & Rubinfeld, 82]

It appears that the variable is a combination of several class-related factors that have been aggregated to a given township. Unfortunatley, these variables are not able to be used as individual predictors, but a strong negative relationship between the % of "lower class" people in the town and the median value exists.


### Judgement Call


There are several reasons as to why there are clusters of neighborhoods at $50k. It's possible that the researchers had blank values and filled those in with the maximum value they had data for. It's also possible that the original researchers decided to filter out any neighborhoods that were above 50k, which could make sense for their research. 

For the purposes of our research, we will be *ignoring any of these neighborhoods valued at cmedv = $50k*. Further research might yield insight into why those neighborhoods look this way, and a published paper should try to do so, but for this assignment we'll keep our dataset within those bounds. We lose 16 neighborhoods doing so, or around 3% of our data. 

```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
data <- data %>% 
  filter(cmedv<50)
```


# Part 2: Regression

To start with, we'll run a very basic linear model with all the other predictors in it to see our benchmark of performance with no changes or alterations. 


### Benchmark Model Summary 
```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
benchmark <- lm(cmedv ~ crim + zn + indus + chas + nox + rm + age + log(dis) + rad + tax + ptratio + lstat + black_neighborhood, data = data)

summary(benchmark)
```

Our benchmark adjusted R-squared suggests that about 78% of the variance in our data is explained by our model. Our F-statistic is large and the relative p-value suggests the following result to a hypothesis test:

$H_0:$ *There is no difference between our model and the intercept alone.*

$H_A:$ *There is difference between our model and the intercept alone.*

Since our p-value is below .05, we can reject the null and conclude that some combination of the above predictors improves the model beyond the intercept alone. 

To help us validate that this model is working, we look at the residual values. 

```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
# plot our residual plot
ggplot(benchmark, aes(x = .fitted, y = .resid)) +
  geom_point()+
  labs(title='Residual Plot for Benchmark Model')
```

Next, we need to validate that our model complies with the following assumptions: 
1. Is the variance of our residuals consistent?
2. Is the mean of our Residuals 0? 


There are other assumptions we can verify as well, but we'll start with these since we can identify issues with these using just the residual plot. From observing the residual plot, we can see that none of the above assumptions are met completely. The variance in our residuals seems to fluctuate as our fitted values increase. Likewise, the variance is not centered around 0 and a slight curve to a line is appearing, violating assumptions 2. 

### Transformations

Normally we would consider transforming the response variable first, since transforming the predictor does not affect the variance of the error terms and any transformations we do to the predictors might be skewed if we need to eventually transform the response. But the above suggests a curve rather than wild variance, so we'll start with predictors. 

Since we did a log transformation on the crime rates when comparing them to cmedv, we'll start with seeing how well that improves the residuals for a simple model using just crime as a predictor. 

```{r, echo=FALSE,  out.width="100%", out.height="50%", fig.align = 'center'}
# A simple linear model with just the crime vs cmedv. 
SLR_crim <- lm(cmedv ~ crim, data = data)
SLR_crim_log <- lm(cmedv ~ log(crim), data = data)
p9 <- ggplot(SLR_crim, aes(x = .fitted, y = .resid)) +
  geom_point(color='darkred')+
  labs(title='Residual Plot for Value ~ Crime')
p10 <- ggplot(SLR_crim_log, aes(x = .fitted, y = .resid)) +
  geom_point(color='blue3')+
  labs(title='Residual Plot for Value ~ Log(Crime)')
# Improves things phenomenally. The leftover increase in variance is negligible.
grid.arrange(p9,p10,
  ncol = 2,
  clip = FALSE
)
```

This is a large improvement. Let's also look at the `Distance to Work`, `House Age`, and `% Lower Class` features next, as they also seem to follow none-linear patterns. It's not inherently clear which one will work with which, so we'll use BoxCox to help us figure out which of these variables should be transformed in which ways. 

Below, we'll show an example of one of several BOXCOX plots we used to help make our decisions. The other plots were very similar to this one below, that shows the boxcox plot for `cmedv` predicted by the `lstat` predictor, which measure the % of the population in "lower class". 

### BOXCOX for cmedv ~ Class
```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
boxcox(lm(cmedv ~ lstat, data=data))
```

Since the lambda variable is optimized *right* next to 0, we'll try a log transformation for this variable. The others that were closer to .5, we tried a square-root transformation instead. The BOXCOX plot is really a guide to help us figure out where to look, but the end decision we made by looking at how our residuals looked once we've made these transformations.

```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
transformed <- lm(cmedv ~ log(crim) + zn + indus + chas + nox + rm + sqrt(age) + sqrt(dis) + rad + tax + ptratio + log(lstat) + black_neighborhood, data = data)
# plot our residual plot
ggplot(transformed, aes(x = .fitted, y = .resid)) +
  geom_point()+
  labs(title='Residual Plot for Transformed Model')
```


The above fitted residuals look much better. There's very little curvature in the residuals nor does there appear to be any stark increase or decrease in the variance of the residuals. One thing to note is that if our goals were to maximize the accuracy of predictions, we might care less about the interpretability of our predictor and response variables and therefore we might transform our response variable as well to see how it affects performance. Since we do care about interpretability, this is where we'll stop our transformations and move on.

### Two more assumptions to check for

First, let's check the QQ norm plot to see how well our residuals follow the assumed normal distribution. 

```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
# qqnorm
qqnorm(transformed$residuals)
qqline(transformed$residuals, col="red")
```

There is room for improvement, but since this particular assumption is not the most important for us to bend to and the samples fit close enough to our expectations, we'll move on to checking whether our residuals are independently related to each other by checking the ACF plot below. 

### ACF Plot of our Model Post Transformations

```{r, echo=FALSE}
# ACF Plot
acf(transformed$resid)
```

Interestingly, there does appear to be a slight relationship in one residual from the next. A pattern appears to emerge where one observation seems oddly close to the next one in line. What this might tell us is that there is a relationship between the observations in one township and the neighboring areas. This makes sense logically, as one expensive town is more likely to be neighbored by another expensive town rather than completely random placements of high and low value towns sporadically throughout Boston. 

There is little we can do about this relationship and the data we have thus far, so while our assumption is not met to 100% our satisfaction, we feel comfortable moving forward with the data we have so far. We will be sure to note this discrepancy in our final conclusions. 

```{r, include=FALSE}
regnull<-lm(cmedv~log(crim), data= data)
regfull<-lm(cmedv ~ log(crim) + zn + indus + chas + nox + rm + sqrt(age) + sqrt(dis) + rad + tax + ptratio + log(lstat) + black_neighborhood, data= data)
step(regnull, scope=list(lower=regnull, upper=regfull), direction = "forward")
```

```{r, include=FALSE}
step(regfull, scope=list(lower=regnull, upper=regfull), direction = "backward")
```


We ran both forwards and backwards predictor selections on our model. Both selection processes selected the same variables. The variables removed are chas, zn, indus, and sqrt(age). 

#### Bonferroni Outlier Detection
```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
result <- lm(cmedv ~ log(crim) + nox + rm + sqrt(dis) + rad + 
    tax + ptratio + log(lstat) + black_neighborhood, data = data)
res <-result$residuals
student.res<-rstandard(result)
ext.student.res<-rstudent(result)
# Outlier detection using Bonferroni
n<- dim(data)[1]
p<- 19
crit<-qt(1-0.05/(2*n), n-p-1)
ext.student.res[abs(ext.student.res)>crit]
```

By using Bonferroni method for outlier detection, we find that observation 368 in our dataset is an outlier.

#### High-Leverage Points

```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
lev <- lm.influence(result)$hat
lev[lev>2*p/n]
```


We identify the high leverage observations, which are the data points that are the most influencial. We ran leverages analysis, Cook's distance, and DFFITs to find the most influcencial data point. The outlier observation 368 was repeatedly included as an influcencial observation, however, Cook's distance resulted in no values. Though observation 368 is of concern, the analysis indicates that there is only one true outlier. 

#### Cook's Distance
```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
COOKS<-cooks.distance(result)
COOKS[COOKS>qf(0.5,p,n-p)]
```


```{r, include=FALSE}
DFFITS<-dffits(result)
DFFITS[abs(DFFITS)>2*sqrt(p/n)]
```

### What does Our Outlier Look Like?

Upon closer inspection of this outlier point, we don't observe any notable reason to drop this neighborhood from our dataset. Our final coefficient of interest (`black_neighborhood`) is largely unaffected by the addition of this point, and since the model's performance is still performing better than our benchmark, we'll include it in our final model as well. 

```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
data[c(368), ]
```

### Final MLR Model Coefficients and Performance

``` {r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
summary(result)
```


Above we can see that the coefficient for `black_neighborhood` is - 3.4, which equates to a drop in expected median housing value by about $3,400 if the neighborhood is considered black, based on our assumptions and in the presence of the other predictors we've included into our final model.  

### Confindence Interval for `black_neighborhood`

```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
confint(result)['black_neighborhood1',]
```

We can be 95% confident that the true impact of a neighborhood being a black neighborhood is between -\$1,724 and -\$4,366. This is a good step towards providing evidence of racial disparities in housing values, but one additional step we can take is to see if we can predict the type of neighborhood a tract is (black or non) based on the median value or if other predictors account for more value in the next model.


# Part 4: Logistic Regression

We wanted to see if a logistic model could be useful in predicting whether a neighborhood was a black neighborhood.

Similar to the multiple regression model, we will run a basic logistic model with all the predictors included. This will give us a sense of our benchmark, as well as which predictors could be useful. We will also split the data set into two: 75% for training and 25% for testing. We will use the training data frame to create a model and the testing data frame to measure it's performance.

### Benchmark Model Summary 
```{r,message= FALSE, warning = F, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
set.seed(100) ##for reproducibility to get the same split
sample<-sample.int(nrow(data), floor(.75*nrow(data)), replace = F)
train<-data[sample, ] ##training data frame
test<-data[-sample, ] ##test data frame
#logistic regression with all variables
result.full<-glm(black_neighborhood~cmedv + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + lstat +crim, family = "binomial", data=train)
summary(result.full)
```

It appears that all of the variables except cmedv (median home value) and rm (average number of rooms) are not significant. Since we've already confirmed a relationship between average home value and black neighborhood in Part 1, it's not surprising to see a similar relationship in our logistic model.  Let's chart the variables for average number of rooms and black neighborhood to see if there's a relationship. 



```{r, echo=FALSE}
ggplot(data, aes(x= black_neighborhood, y=rm))+
  geom_boxplot(fill="light blue")+
  labs(title="Box Plot of Average Number of Rooms By black_neighorhood", y= 'Average Number of Rooms')
```

It seems like the average number of rooms is similar between black neighborhoods and non-black neighborhoods at around 6.25. However, the variance for the average seems to be much higher for non-black neighborhoods. There also seems to be a large cluster of outliers above 7.5 rooms for non-black neighborhoods.

Now, we run a logistic regression on the test data using only the variables for average home value and average number of rooms. 
```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
#logistic regression with only cmedv and rm
result.reduced<-glm(black_neighborhood~cmedv+ rm , family = "binomial", data= train)
summary(result.reduced)
```

The reduced model looks much better since the p-values for both coefficients are close to zero. The coefficient for `rm` is positive, which indicates that holding the average home value constant, increasing the average number of rooms increases the probability that the neighborhood is black. The coefficient for the average home value is negative. This indicates that holding the average number of rooms constant, increasing the average home value decreases the probability that the neighborhood is black. 

We will now perform the following hypothesis test to see whether we should go with the full model or reduced model:  

$H_o:$ *Betas for all coefficients except for cmedv and rm are equal to zero *

$H_a:$ *At least one of the coefficients in H0 is not zero*

```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
#test statistic
deltaG2_partial<-result.reduced$deviance-result.full$deviance
#deltaG2_partial

#p-value
p<-1-pchisq(deltaG2_partial,10)
sprintf('p-value = %e', p)
```

The p-value of 0.111 is higher than our alpha of 0.05. Therefore, we fail to reject the null and decide to go with the reduced model.

Next, we test how our reduced logistic model with two variables perform against the testing data frame. We also perform the following hypothesis test to determine if we should use the full model or reduced model.  

$H_o:$ *$\beta_1$ = $\beta_2$ = .. = $\beta_12$ = 0*

$H_a:$ *At least one of the coefficients in H0 is not zero*

```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
deltaG2<-result.reduced$null.deviance-result.reduced$deviance
p<- 1-pchisq(deltaG2,12)
sprintf('p-value = %e', p)
```

Since the p-value is less than our alpha of 0.05, we reject the null and conclude that the reduced model is useful compared to the intercept only model.

Let's now see how our reduced logistic model with two variables perform against the testing data frame. We'll plot the ROC curve and then calculate the AUC.


```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
##predictions for black_neighborhood variable for test data based on training data
preds<-predict(result.reduced,newdata=test, type="response")
rates<-prediction(preds, test$black_neighborhood)
##store the true positive and false positive rates
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
##plot ROC curve and overlay the diagonal line for random guessing
plot(roc_result, main="ROC Curve")
lines(x = c(0,1), y = c(0,1), col="red")
#3.
##compute the AUC
auc<-performance(rates, measure = "auc")
sprintf('AUC = %.4f',auc@y.values)
```

The ROC curve is well above the diagonal line except for at the very end. It looks like the logistic regression performs much better than random guessing. 


This is also confirmed by a strong AUC of 0.8596. Since the AUC is well above 0.5, we can conclude that the model does better than random guessing and does have predictive value.


We also want to calculate where in the ROC curve a 50% threshold would lie. We'll create a confusion matrix to calculate the TPR and FPR to find the point.

```{r, echo=FALSE,  out.width="75%", out.height="50%", fig.align = 'center'}
#confusion matrix
tab<-table(test$black_neighborhood, preds>0.5)
tab
#accuracy
acc<-(tab[1,1]+tab[2,2])/(tab[1,1]+ tab[2,2] +tab[1,2]+ tab[2,1])
#calculating true positive rate
tpr<-tab[2,2]/ (tab[2,2]+tab[2,1])
#calculating false positive rate
fpr<- tab[1,2]/ (tab[1,1]+tab[1,2])
sprintf('FPR = %.2f', fpr)
sprintf('TPR = %.2f', tpr)
```

The point (0,0.44) lies well above the diagonal and therefore the model is a useful predictor when using a 50% threshold.


# Part 5: Conclusions


Both models show evidence of impact on housing prices in Boston related to the race of the population in that neighborhood. In the presence of other predictors, our first model suggests that a black neighborhood could experience between \$1,700 to \$4,300 difference in housing prices. 

While our analysis does not conclude that the racial disparities we found were a direct result of the racist policies that were active in Boston at the time of the data collection, our research provides evidence that there could be a connection and should be considered along with other corroborating evidence. The conclusion we made when validating the assumptions - that wealthy neighborhoods neighbored other wealthy neighborhoods, could also implicate these policies. Areas in the city were historically designed to be predominantly white or predominantly non-white prior to our dataset being collected. 
	
The racial disparity we have found is not only a snapshot of its time. These housing disparities could still exist today, and further research can illuminate how much or how little this has changed over time. 




# Bibliography
Michael Carlisle . “racist data destruction?” Medium, 13 June. 2019, https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8. Accessed 15 November. 2021.

David Harrison, Daniel L Rubinfeld, “Hedonic housing prices and the demand for clean air”,
Journal of Environmental Economics and Management,Volume 5, Issue 1,1978,Pages 81-102, ISSN 0095-0696.
