---
title: 'Project 2: Group 1 - Boston Housing Racial Bias'
author: "Lauren Bassett, Will Johnson, Anoop Nath, Aishwarya Pradhan"
date: "12/3/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
# install.packages("gridExtra")
# install.packages("ROCR")
library(gridExtra)
library(ROCR)
library(MASS)
#?BostonHousing2
```

# Exploring our Dataset

```{r}
# Download our dataset
library(mlbench)
data(BostonHousing2)
data <- BostonHousing2
head(data)
```



## "Majority Black" response variable

The variable "b" is described in the census documents as...

> 1000(B - 0.63)^2 where B is the proportion of blacks by town
Initially, our goal was to categorize the data using a “majority black” binary variable. Based on the description, we thought the values would be between 0 and 1, or possibly 0 and 100. Instead, the variable is distributed on a quadratic curve. 

```{r}
# plot the distribution on the "b" variable. 
ggplot(data, aes(x = b))+
  geom_histogram(binwidth = 1)
```
The proportion variable ranges from 0 to 400, and most of the data has values on the upper end of the range. The data has been transformed so that higher values represent neighborhoods with higher white populations. 
	
If we try to reverse engineer the original B value (based on the formula b=1000(B-.63)2), it’s not possible to get the answer. The result of the previously mentioned formula is a quadratic curve, plotted below:


```{r}
# Using the dataset's documentation, plot potential values for proportions of black people
# against each value of `data$b`. 
props <- c()
i <- .01
for(x in 1:99){
    props <- c(props,1000*(i-.63)**2 )
    i = i + .01
}
parab <- data.frame(props,seq(.01, .99, by=.01))
colnames(parab) = c('x','y')
ggplot(data = parab, aes(x=y, y=x ))+
  geom_line()+
  labs(x = "Potential proportion of black people", y='b variable')
```

It is impossible to retrieve the original B variable because there are two possible values of B for each transformed variable. Neighborhoods with a black population higher than 26% become harder to determine. 
	

Some brief external research suggests that this transformation on the variable was set to create a pseudo-parabolic relationship to account for an initial drop in value when the proportion was too mixed-race. After 75% black, the effect was expected to rise again because of preference for a neighborhood of one’s own race or “self-segregation” [Harrison,Rubinfeld, 96].
	

Researcher Michael Carlisle, an assistant professor of Mathematics at CUNY, said:

> Harrison and Rubinfeld appear to have decided on a threshold of 63% at which to switch the regime of price decline to price increase (i.e. a so-called “ghetto threshold”) [Carlisle,1]
	
We will continue to follow the precedent they set in the 1970’s. We’ll assess if there is a relationship between where a home falls on that threshold to see its effect on the price of a home, as well as try to predict where on that threshold a house will fall for our logistic model.

Plugging our 26% threshold into that formula, we get:

$1000*(0.26 - 0.63)2=136.9$

so black_neighborhood = 1 if b <= 136.9

```{r}
# code to create binary black_neighborhood variable
data <- data %>% 
           mutate(black_neighborhood = (b <= 136.9) * 1)
# what % of our neighborhoods are black neighborhoods by this definition? 
sum(data$black_neighborhood)/nrow(data)
```
After generating the binary variable, we generated visualizations to see how the other variables interact. Specifically, we are focused on the price variable. 

```{r}
data$black_neighborhood <- as.factor(data$black_neighborhood)
# plot our new variable and the price distributions across it. 
ggplot(data, aes(x= black_neighborhood, y=medv))+
  geom_boxplot(fill="light blue")
```


The visualizations above show a relationship between neighborhoods that are predominately black and the price of housing. Thus, we can assume that the black neighborhood indicator can be used as a predictor for housing prices. 




# Part 1 - EDA  : How much does the relative blackness of a neighborhood affect price? 

First, we will generate views to determine what other predictors affect the price of housing. We begin by looking at the distribtion of house prices across the entire dataset. 

```{r}
ggplot(data, aes(x=cmedv))+
  geom_histogram()+
  labs(title = "Distribution of Median Housing Values per Area")
```


The price of housing appears normally distributed. There are a few towns with noticably higher prices. These may be potential outliers in the dataset.  

Next, we generate scatterplots to describe the quantiative variables. 



```{r}
ggplot(data, aes(x = log(crim), y = cmedv))+
  geom_point(color = 'green2')+
  labs(title="Crime Rate (log) by Median Home Value",
       x = "Crime Rate - Log Scale")
```

It appears that as crime rate increases, the median value of the home decreases. However, as most areas in the data set had extremely low crime rates, we used a log scale for crime to better highlight this trend. It is important to note that the relationship between crime and housing prices is not constant, as there are some expensive areas in the data set that have relatively higher crime rates per capita. 


Next, we assess how the size of the home, approximated by the number of rooms, affects the median price. 


```{r}
ggplot(data, aes(x = rm, y = cmedv))+
  geom_point(color = 'green3')+
  labs(title="Room Count avg by Median Home Value",
       x = "Average Number of Rooms")
```

The relationship between number of rooms and median price is strong. However, there are some high-leverage points (by observation) where the median home value is around $50,000 but the average number of rooms is far lower. There may be other predictors that are influencing these high-leverage points.  

One possible confounding variable could be the industry proportion. We believe business neighborhoods with high expenses may also have relatively high crime rates.

```{r}
ggplot(data, aes(x = indus, y = cmedv))+
  geom_point(color = 'green4')+
  labs(title="Proportion of non-retail business by Median Home Value",
       x = "% of non-retail business acres per town")
```



```{r}
p1 <- ggplot(data, aes(x = nox, y = cmedv))+
      geom_point(color = 'purple1')+
      labs(title="NO2 concentration by Med. Value",
           x = "Nitric Oxides Conentration (parts per 10m)")
p2 <- ggplot(data, aes(x = dis, y = cmedv))+
      geom_point(color = 'purple2')+
      labs(title="Distance to Work by Med. Value",
           x = "Weighted Distances to Employment Centres")
p3 <- ggplot(data, aes(x = rad, y = cmedv))+
      geom_point(color = 'purple3')+
      labs(title="Highway Access by Med. Value",
           x = "Index of Accessibility to Highways")
p4 <- ggplot(data, aes(x = tax, y = cmedv, color=rad))+
      geom_point(alpha=.5)+
      labs(title="Property Tax Rate by Med. Value",
           x = "Property Tax rate per $10,000")
# Proportions
p5 <- ggplot(data, aes(x = ptratio, y = cmedv))+
      geom_point(color = 'orange2')+
      labs(title="Student-Teacher Ratio by Med. Value",
           x = "Studnets per Teacher")
p6 <- ggplot(data, aes(x = age, y = cmedv))+
      geom_point(color = 'orange3')+
      labs(title="Age by Med. Value",
           x = "% of homes build prior to 1940")
p7 <- ggplot(data, aes(x = lstat, y = cmedv))+
  geom_point(color = 'orange4')+
  labs(title="% of Lower Class by Med. Value",
       x = "% of 'Lower Status' Citizens")
# Boolean Variables
p8 <- ggplot(data, aes(x = chas, y = cmedv))+
      geom_boxplot(fill = "lightblue2")+
      labs(title="House By the River (y/n) by Med. Value",
           x = "Homes Line Charles River")
grid.arrange(p1,p2,p3,p4,
  ncol = 2,
  clip = TRUE
)
grid.arrange(p5,p6,p7,p8,
  ncol = 2,
  clip = FALSE
)
```

The views reveal key insights about our data set. The Tax rate chart divides the dataset into two. The only other predictor that does this is the Index of Radial Highway access. Layering both together into the same chart via color, you can see that the group in the highest tax bracket is entirely made up of those areas with close access to the highways. Accessibility to highways seems to create a divide between groups in our dataset.

Other relationships of note are the positive relationship between proximity to the river on price, and the negative relationship between age of the home on price. Finally, there's this indexed metric called `lstat` which measures the "Percentage of Lower Status of the Population". Looking further into the 1970 Census paper, it looks like this metric measures the following:

"""
> Proportion of population that is lower status = 1/2 (proportion of adults without, some high school education and proportion of male workers classified as laborers). The logarithmic specification implies that socioeconomic status distinctions mean more in the upper brackets of society than in the lower classes. Source: 1970 U. S. Census
[Harrison & Rubinfeld, 82]
"""
It appears that the variable is a combination of several class-related factors that have been aggregated to a given township. Unfortunatley, these variables are not able to be used as individual predictors, but there is a strong negative relationship between the % of "lower class" people in the town and the median value.
### Judgement Call

There are several reasons as to why there are clusters of neighborhoods at $50k. It's possible that the researchers had blank values and filled those in with the maximum value they had data for. It's also possible that the original researchers decided to filter out any neighborhoods that were above 50k, which could make sense for their research. 

For the purposes of our research, we will be *ignoring any of these neighborhoods valued at cmedv = $50k*. Further reserach might yield insight into why those neighborhoods look this way and a published paper should try to do so, but for this assignment we'll keep our dataset within those bounds. We lose 16 neighborhoods doing so, or around 3% of our data. 

```{r}
data <- data %>% 
  filter(cmedv<50)
```


# Part 2: Regression

To start with, we'll run a very basic linear model with all the other predictors in it to see our benchmark of performance with no changes or alterations. 


### Benchmark Model Summary 
```{r}
benchmark <- lm(cmedv ~ crim + zn + indus + chas + nox + rm + age + log(dis) + rad + tax + ptratio + lstat + black_neighborhood, data = data)
```

With the above benchmark set, it appears we are off to a good start. Our adjusted R-squared suggests that nearly 74% of the variance in our data is explained by our model. Our F-statistic is large and the relative p-value suggests the following result to a hypothesis test:

$H_0:$ *There is no difference between our model and the intercept alone.*
$H_A:$ *There is difference between our model and the intercept alone.*

Since our p-value is below .05, we can reject the null and conclude that some combination of the above predictors improves the model beyond the intercept alone. 

To help us validate that this model is working, we look at the residual values. 

```{r}
# plot our residual plot
ggplot(benchmark, aes(x = .fitted, y = .resid)) +
  geom_point()+
  labs(title='Residual Plot for Benchmark Model')
```

Next, we need to validate that our model complies with the following assumptions: 
1. Is the variance of our residuals consistent?
2. Is the mean of our Residuals 0? 


There are other assumptions we can verify as well, but we'll start with these since we can identify issues with these using just the residual plot. From observing the residual plot, we can see that none of the above assumptions are met completely. The variance in our residuals seems to fluctuate as our fitted values increase. Likewise, the variance is not centered around 0 and a slight curve to a line is appearing, violating assumptions 2. 

### Transformations

Ideally, we won't need to transform our response variable at all because we're hoping to interpret coefficients directly. Normally we would consider transforming the response variable first, since transforming the predictor does not affect the variance of the error terms and any transformations we do to the predictors might be skewed if we need to eventually transform the response. But the above suggests a curve rather than wild variance, so we'll start with predictors. 

Since we did a log transformation on the crime rates when comparing them to cmedv, we'll start with seeing how well that improves the residuals for a simple model using just crime as a predictor. 

```{r}
# A simple linear model with just the crime vs cmedv. 
SLR_crim <- lm(cmedv ~ crim, data = data)
SLR_crim_log <- lm(cmedv ~ log(crim), data = data)
p9 <- ggplot(SLR_crim, aes(x = .fitted, y = .resid)) +
  geom_point(color='darkred')+
  labs(title='Residual Plot for Value ~ Crime')
p10 <- ggplot(SLR_crim_log, aes(x = .fitted, y = .resid)) +
  geom_point(color='blue3')+
  labs(title='Residual Plot for Value ~ Log(Crime)')
# Improves things phenomenally. The leftover increase in variance is negligible.
grid.arrange(p9,p10,
  ncol = 2,
  clip = FALSE
)
```
This is a large improvement. Let's also look at the `Distance to Work`, `House Age`, and `% Lower Class` features next, as they also seem to follow none-linear patterns. It's not inherently clear which one will work with which, so we'll use BoxCox to help us figure out which of these variables should be transformed in which ways. 

Below, we'll show an example of one of several BOXCOX plots we used to help make our decisions. The other plots were very similar to this one below, that shows the boxcox plot for `cmedv` predicted by the `lstat` predictor, which measure the % of the population in "lower class". 

### BOXCOX for cmedv ~ Class
```{r}
boxcox(lm(cmedv ~ lstat, data=data))
```

Since the lambda variable is optimized *right* next to 0, we'll try a log transformation for this variable. The others that were closer to .5, we tried a square-root transformation instead. The BOXCOX plot is really a guide to help us figure out where to look, but the end decision we made by looking at how our residuals looked once we've made these transformations.

```{r}
transformed <- lm(cmedv ~ log(crim) + zn + indus + chas + nox + rm + sqrt(age) + sqrt(dis) + rad + tax + ptratio + log(lstat) + black_neighborhood, data = data)
# plot our residual plot
ggplot(transformed, aes(x = .fitted, y = .resid)) +
  geom_point()+
  labs(title='Residual Plot for Transformed Model')
```


The above fitted residuals look much better. There's very little curvature in the residuals nor does there appear to be any stark increase or decrease in the variance of the residuals. One thing to note is that if our goals were to maximize the accuracy of predictions, we might care less about the interpretability of our predictor and response variables and therefore we might transform our response variable as well to see how it affects performance. Since we do care about interpretability, this is where we'll stop our transformations and move on.

### Two more assumptions to check for

First, let's check the QQ norm plot to see how well our residuals follow the assumed normal distribution. 

```{r}
# qqnorm
qqnorm(transformed$residuals)
qqline(transformed$residuals, col="red")
```

There is room for improvement, but since this particular assumption is not the most important for us to bend to and the samples fit close enough to our expectations, we'll move on to checking whether our residuals are independently related to each other by checking the ACF plot below. 

### ACF Plot of our Model Post Transformations

```{r}
# ACF Plot
acf(transformed$resid)
```

Interestingly, there does appear to be a slight relationship in one residual from the next. A pattern appears to emerge where one observation seems oddly close to the next one in line. What this might tell us is that there is a relationship between the observations in one township and the neighboring areas. This makes sense logically, as one expensive town is more likely to be neighbored by another expensive town rather than completely random placements of high and low value towns sporadically throughout Boston. 

There is little we can do about this relationship and the data we have thus far, so while our assumption is not met to 100% our satisfaction, we feel comfortable moving forward with the data we have so far. We will be sure to note this discrepancy in our final conclusions. 



# Part 4: Logistic Regression

We wanted to see if a logistic model could be useful in predicting whether a neighborhood was a black neighborhood.

Similar to the multiple regression model, we will run a basics logistic model with all the predictors included. This will give us a sense of our benchmark, as well as which predictors could be useful. We will also split the data set into two: 75% for training and 25% for testing. We will use the training data frame to create a model and the testing data frame to measure it's performance.

### Benchmark Model Summary 
```{r,message= FALSE}
set.seed(100) ##for reproducibility to get the same split
sample<-sample.int(nrow(data), floor(.75*nrow(data)), replace = F)
train<-data[sample, ] ##training data frame
test<-data[-sample, ] ##test data frame
#logistic regression with all variables
result.full<-glm(black_neighborhood~cmedv + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + lstat , family = "binomial", data=train)
summary(result.full)
```

It appears that all of the variables except cmedv (median home value) and rm (average number of rooms) are not significant. Since we've already confirmed a relationship between average home value and black neighborhood in Part 1, it's not surprising to see a similar relationship in our logistic model.  Let's chart the variables for average number of rooms and black neighborhood to see if there's a relationship. 



```{r}
ggplot(data, aes(x= black_neighborhood, y=rm))+
  geom_boxplot(fill="light blue")+
  labs(title="Box Plot of Average Number of Rooms By black_neighorhood", y= 'Average Number of Rooms')
```

It seems like the average number of rooms is similar between black neighborhoods and non-black neighborhoods at around 6.25. However, the variance for the average seems to be much higher for non-black neighborhoods.There also seems to be a large cluster of outliers above 7.5 rooms for non-black neighborhoods.

Now, we run a logistic regression on the test data using only the variables for average home value and average number of rooms. 
```{r}
#logistic regression with only cmedv and rm
result.reduced<-glm(black_neighborhood~cmedv+ rm , family = "binomial", data= train)
summary(result.reduced)
```

The reduced model looks much better since the p-values for both coefficients are close to zero. It's interesting to see that the coefficient for rm is positive. This indicates that holding the average home value constant, increasing the average number of rooms increases the probability that the neighborhood is black. 

The coefficient for the average home value is negative. This indicates that holding the average number of rooms constant, increasing the average home value decreases the probability that the neighborhood is black. 

We will now perform the following hypothesis test to see whether we should go with the full model or reduced model:  

$H_o:$ *Betas for all coefficients except for cmedv and rm are equal to zero *
$H_a:$ *At least one of the coefficients in H0 is not zero*

```{r}
#test statistic
deltaG2_partial<-result.reduced$deviance-result.full$deviance
deltaG2_partial

#p-value
1-pchisq(deltaG2_partial,10)
```

The p-value of 0.111 is higher than our alpha of 0.05. Therefore, we fail to reject the null and decide to go with the reduced model.

Let's now see how our reduced logistic model with two variables perform against the testing data frame. We'll also perform the following hypothesis test to see whether we should go with the full model or reduced model.  


```{r,message= FALSE}
##predictions for black_neighborhood variable for test data based on training data
preds<-predict(result.reduced,newdata=test, type="response")
rates<-prediction(preds, test$black_neighborhood)
##store the true positive and false positive rates
roc_result<-performance(rates,measure="tpr", x.measure="fpr")
##plot ROC curve and overlay the diagonal line for random guessing
plot(roc_result, main="ROC Curve")
lines(x = c(0,1), y = c(0,1), col="red")
#3.
##compute the AUC
auc<-performance(rates, measure = "auc")
auc@y.values
```
We'll plot the ROC curve and then calculate the AUC.


The ROC curve is well above the diagonal line except for at the very end. It looks like the logistic regression performs much better than random guessing. 


This is also confirmed by a very strong AUC of 0.8596. Since the AUC is well above 0.5, we can conclude that the model does better than random guessing and does have predictive value.



```{r}
regnull<-lm(cmedv~log(crim), data= data)
regfull<-lm(cmedv ~ log(crim) + zn + indus + chas + nox + rm + sqrt(age) + sqrt(dis) + rad + tax + ptratio + log(lstat) + black_neighborhood, data= data)
step(regnull, scope=list(lower=regnull, upper=regfull), direction = "forward")
```

```{r}
step(regfull, scope=list(lower=regnull, upper=regfull), direction = "backward")
```

Through our analysis of doing forward and backward regression on the transformed variables, we find that the black_neighborhood1 variable was retained and relevant in both models. Therefore we can posit that the indicator of the predominantly black neighborhood has an influence on housing prices in Boston.  


```{r}
result <- lm(cmedv ~ log(crim) + nox + rm + sqrt(dis) + rad + 
    tax + ptratio + log(lstat) + black_neighborhood, data = data)
res <-result$residuals
student.res<-rstandard(result)
ext.student.res<-rstudent(result)
# Outlier detection using Bonferroni
n<- dim(data)[1]
p<- 19
crit<-qt(1-0.05/(2*n), n-p-1)
ext.student.res[abs(ext.student.res)>crit]
```

By using Bonferroni method for outlier detection, we find that observation 368 in our dataset is an outlier.

```{r}
lev <- lm.influence(result)$hat
lev[lev>2*p/n]
```

We identify the high leverage observations, which are the data points that are the most influencial. We ran leverages analysis, Cook's distance, and DFFITs to find the most influcencial data point. The outlier observation 368 was repeatedly included as an influcencial observation, however, Cook's distance resulted in no values. Though observation 368 is of concern, the analysis indicates that there is only one true outlier. 


```{r}
COOKS<-cooks.distance(result)
COOKS[COOKS>qf(0.5,p,n-p)]
```
```{r}
DFFITS<-dffits(result)
DFFITS[abs(DFFITS)>2*sqrt(p/n)]
```

```{r}
data[c(368), ]
```

# Part 5: Check the prompt for more stuff. 








# Bibliography
Michael Carlisle . “racist data destruction?” Medium, 13 June. 2019, https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8. Accessed 15 November. 2021.

David Harrison, Daniel L Rubinfeld, “Hedonic housing prices and the demand for clean air”,
Journal of Environmental Economics and Management,Volume 5, Issue 1,1978,Pages 81-102, ISSN 0095-0696.
